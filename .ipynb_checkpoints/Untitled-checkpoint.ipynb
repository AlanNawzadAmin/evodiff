{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "14ba08c7-4c3b-4a5d-ad3e-43a154471fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from evodiff.utils import Tokenizer\n",
    "import pathlib\n",
    "from sequence_models.datasets import UniRefDataset\n",
    "from tqdm import tqdm\n",
    "from evodiff.plot import aa_reconstruction_parity_plot\n",
    "import pandas as pd\n",
    "from evodiff.pretrained import load_sequence_checkpoint\n",
    "from matplotlib import pyplot as plt\n",
    "import pkg_resources\n",
    "from evodiff.utils import Tokenizer\n",
    "\n",
    "\n",
    "home = str(pathlib.Path.home())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f4a2f1c1-86aa-4789-8b21-0c66fb4bfc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 4, 8, 16, 32, 64, 128, 1, 2, 4, 8, 16, 32, 64, 128]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "model = ByteNetLMTime()\n",
    "collater = SimpleCollater(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2dcbe7e6-c8c5-4561-aae4-014334fe85f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = UniRefDataset('data/uniref50/', 'train', structure=False, max_len=1024)\n",
    "data_valid = UniRefDataset('data/uniref50/', 'test', structure=False, max_len=1024)\n",
    "\n",
    "D =10\n",
    "seqs = [data_train[i] for i in range(D)]\n",
    "data = collater(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "485d86cc-4066-486c-a567-6ba6100697dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43683359"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "074c8b95-6cff-4d00-99fa-42558251c4f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (543) must match the size of tensor b (10) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[154], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m/vast/aa11803/miniconda3/envs/evodiff/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vast/aa11803/miniconda3/envs/evodiff/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[150], line 67\u001b[0m, in \u001b[0;36mByteNetLMTime.forward\u001b[0;34m(self, x, t, S, input_mask)\u001b[0m\n\u001b[1;32m     64\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedder(x)\n\u001b[1;32m     65\u001b[0m c \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msilu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_encoding(t))\n\u001b[0;32m---> 67\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_mod_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m S \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     bs, seq_len \u001b[38;5;241m=\u001b[39m S\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], S\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (543) must match the size of tensor b (10) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "model(data[0], torch.zeros(D).float(), torch.zeros(data[0].shape).float()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "94787716-a95c-4fcf-9d86-5c354badd603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad(tokenized, value, dim=2):\n",
    "    \"\"\"\n",
    "    Utility function that pads batches to the same length.\n",
    "\n",
    "    tokenized: list of tokenized sequences\n",
    "    value: pad index\n",
    "    \"\"\"\n",
    "    batch_size = len(tokenized)\n",
    "    max_len = max(len(t) for t in tokenized)\n",
    "    if dim == 3: # dim = 3 (one hot)\n",
    "        categories = tokenized[0].shape[-1]\n",
    "        output = torch.zeros((batch_size, max_len, categories)) + value\n",
    "        for row, t in enumerate(tokenized):\n",
    "            output[row, :len(t), :] = t\n",
    "    elif dim == 2: # dim = 2 (tokenized)\n",
    "        output = torch.zeros((batch_size, max_len)) + value\n",
    "        for row, t in enumerate(tokenized):\n",
    "            output[row, :len(t)] = t\n",
    "    else:\n",
    "        print(\"padding not supported for dim > 3\")\n",
    "    return output\n",
    "\n",
    "class SimpleCollater(object):\n",
    "    def __init__(self, tokenizer=Tokenizer()):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, sequences):\n",
    "        tokenized = [torch.tensor(self.tokenizer.tokenize(s)) for s in sequences]\n",
    "        tokenized = _pad(tokenized, self.tokenizer.pad_id)\n",
    "        masks = tokenized != self.tokenizer.pad_id\n",
    "        return tokenized.to(torch.long), masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6fbc0aa7-40e4-4043-9cde-d007703ceb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequence_models.layers import PositionFeedForward\n",
    "from sequence_models.convolutional import ByteNetBlock\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ByteNetLMTime(nn.Module):\n",
    "    \"\"\"Stacked residual blocks from ByteNet paper defined by n_layers\n",
    "\n",
    "         Shape:\n",
    "            Input: (N, L,)\n",
    "            input_mask: (N, L, 1), optional\n",
    "            Output: (N, L, d)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_tokens=31, d_embedding=128, d_model=1024, n_layer=16,\n",
    "                 kernel_size=5, r=128, rank=None, n_frozen_embs=None,\n",
    "                 padding_idx=None, causal=False, dropout=0.1, slim=True, activation='gelu',\n",
    "                 schedule_conditioning=True):\n",
    "        \"\"\"\n",
    "        :param n_tokens: number of tokens in token dictionary\n",
    "        :param d_embedding: dimension of embedding\n",
    "        :param d_model: dimension to use within ByteNet model, //2 every layer\n",
    "        :param n_layers: number of layers of ByteNet block\n",
    "        :param kernel_size: the kernel width\n",
    "        :param r: used to calculate dilation factor\n",
    "        :padding_idx: location of padding token in ordered alphabet\n",
    "        :param causal: if True, chooses MaskedCausalConv1d() over MaskedConv1d()\n",
    "        :param rank: rank of compressed weight matrices\n",
    "        :param n_frozen_embs: number of frozen embeddings\n",
    "        :param slim: if True, use half as many dimensions in the NLP as in the CNN\n",
    "        :param activation: 'relu' or 'gelu'\n",
    "        :param down_embed: if True, have lower dimension for initial embedding than in CNN layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.time_encoding = TimestepEmbedder(d_embedding) # Timestep encoding\n",
    "        self.time_mod_layer = nn.Linear(d_embedding, d_model)\n",
    "        if schedule_conditioning:\n",
    "            self.s_embed_input = TimestepEmbedder(d_model)\n",
    "            self.s_embed_block = TimestepEmbedder(d_embedding)\n",
    "        self.embedder = nn.Embedding(n_tokens, d_model, padding_idx=padding_idx)\n",
    "        log2 = int(np.log2(r)) + 1\n",
    "        dilations = [2 ** (n % log2) for n in range(n_layer)]\n",
    "        print(dilations)\n",
    "        d_h = d_model\n",
    "        if slim:\n",
    "            d_h = d_h // 2\n",
    "        self.layers = nn.ModuleList([\n",
    "            ByteNetBlock(d_model, d_h, d_model, kernel_size, dilation=d, causal=causal, rank=rank,\n",
    "                         activation=activation)\n",
    "            for d in dilations\n",
    "        ])\n",
    "        self.c_mod_layers = nn.ModuleList([nn.Linear(d_embedding, 2*d_model) for d in dilations])\n",
    "        self.dropout = dropout\n",
    "        self.decoder = PositionFeedForward(d_model, n_tokens)\n",
    "        self.last_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, t, S, input_mask=None):\n",
    "        \"\"\"\n",
    "        :param x: (batch, length)\n",
    "        :param y: (batch)\n",
    "        :param input_mask: (batch, length, 1)\n",
    "        :return: (batch, length,)\n",
    "        \"\"\"\n",
    "        x = self.embedder(x)\n",
    "        c = F.silu(self.time_encoding(t))[:, None, :]\n",
    "\n",
    "        x = x + self.time_mod_layer(c)\n",
    "        if S is not None:\n",
    "            bs, seq_len = S.shape[0], S.shape[1]\n",
    "            S_out = F.silu(self.s_embed_input(S.reshape(-1))).reshape(bs, seq_len, -1)\n",
    "            x = x + S_out\n",
    "            \n",
    "            # WIP, this is approximately correct but not thoroughly tested\n",
    "            S_out = F.silu(self.s_embed_block(S.reshape(-1))).reshape(bs, seq_len, -1)\n",
    "            c = c[ + S_out\n",
    "\n",
    "        for layer, c_layer in zip(self.layers, self.c_mod_layers):\n",
    "            x = layer(x, input_mask=input_mask)\n",
    "            c_mod = c_layer(c)\n",
    "            modulate_fused(x, *c_mod.chunk(2, dim=-1))\n",
    "            if self.dropout > 0.0:\n",
    "                x = F.dropout(x, self.dropout)\n",
    "        return self.decoder(self.last_norm(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1c15641b-13c0-4aeb-8fc1-0030492761dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# function overload\n",
    "def modulate(x: torch.Tensor,\n",
    "             shift: torch.Tensor,\n",
    "             scale: torch.Tensor) -> torch.Tensor:\n",
    "  return x * (1 + scale) + shift\n",
    "\n",
    "@torch.jit.script\n",
    "def modulate_fused(x: torch.Tensor,\n",
    "                   shift: torch.Tensor,\n",
    "                   scale: torch.Tensor) -> torch.Tensor:\n",
    "  return modulate(x, shift, scale)\n",
    "\n",
    "class TimestepEmbedder(nn.Module):\n",
    "  \"\"\"\n",
    "  Embeds scalar timesteps into vector representations.\n",
    "  \"\"\"\n",
    "  def __init__(self, hidden_size, frequency_embedding_size=256):\n",
    "    super().__init__()\n",
    "    self.mlp = nn.Sequential(\n",
    "      nn.Linear(frequency_embedding_size, hidden_size, bias=True),\n",
    "      nn.SiLU(),\n",
    "      nn.Linear(hidden_size, hidden_size, bias=True))\n",
    "    self.frequency_embedding_size = frequency_embedding_size\n",
    "\n",
    "  @staticmethod\n",
    "  def timestep_embedding(t, dim, max_period=10000):\n",
    "    \"\"\"\n",
    "    Create sinusoidal timestep embeddings.\n",
    "    :param t: a 1-D Tensor of N indices, one per batch element.\n",
    "                      These may be fractional.\n",
    "    :param dim: the dimension of the output.\n",
    "    :param max_period: controls the minimum frequency of the embeddings.\n",
    "    :return: an (N, D) Tensor of positional embeddings.\n",
    "    \"\"\"\n",
    "    # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(\n",
    "      - math.log(max_period)\n",
    "      * torch.arange(start=0, end=half, dtype=torch.float32)\n",
    "      / half).to(device=t.device)\n",
    "    args = t[:, None].float() * freqs[None]\n",
    "    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "    if dim % 2:\n",
    "      embedding = torch.cat(\n",
    "        [embedding,\n",
    "         torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "    return embedding\n",
    "\n",
    "  def forward(self, t):\n",
    "    t_freq = self.timestep_embedding(t, self.frequency_embedding_size)\n",
    "    t_emb = self.mlp(t_freq)\n",
    "    return t_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8928e09-4b1b-4b12-9ef2-270324c0d0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evodiff",
   "language": "python",
   "name": "evodiff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
